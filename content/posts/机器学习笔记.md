---
title: '机器学习笔记'
date: 2025-03-22T11:14:03+08:00
draft: false
---

# 误差反向传播

误差反向传播是通过计算损失函数对网络中每个权重的梯度，并利用梯度下降法更新权重，以最小化预测误差的过程。它结合了**链式法则（微积分中的求导规则）**和**梯度下降法**，实现对多层神经网络的高效训练。

如果不能理解，可以去看看[深度学习入门](https://book.douban.com/subject/30270959/)，这本书中作者使用了简单的例子，利用**计算图**的方式说明了误差反向传播。当理解了误差反向传播后，需要通过代码加深理解，可以参考[深度学习入门2:自制框架](https://book.douban.com/subject/36303408/)以及[Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy)大神的视频[The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0)。

# 神经网络

初始损失值问题

通常在进行神经网络训练的时候，对于权重和bias初始化都是随机初始化的，这可能会导致初始损失值会非常大。比如基于character进行预测的时候，初始情况我们是希望所以character出现的概率是一样的，即损失值是一个比较小的值，但是如果是权重和bias初始化都是随机初始化即可能会导致初始损失值非常大的情况。

解决思路：对于影响初始损失值的bias，我们可以使用0进行初始化。对于权重我们可以乘一个系数比如0.01。这样可以在一定程度上减小初始损失值过大的问题。这样神经网络前面的训练就不是在压缩权重，将更多的计算资源用在真正需要调整的计算上，进而提高神经网络的训练效率。

可以参考[Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy)大神的视频的[Building makemore Part 3: Activations & Gradients, BatchNorm](https://www.youtube.com/watch?v=P6sfmUTpUmc)的[fixing the initial loss](https://www.youtube.com/watch?v=P6sfmUTpUmc&t=259s)部分

# Transformer

机器学习的经典论文[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)提到的Transformer架构，可以参考学习[李沐](https://www.youtube.com/@mu_li)大神的视频[Transformer论文逐段精读](https://www.youtube.com/watch?v=nzqlFIcCSWQ)
